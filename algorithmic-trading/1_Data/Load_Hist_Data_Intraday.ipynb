{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data from Deutsche BÃ¶rse Public Dataset into S3 Bucket\n",
    "\n",
    "This notebook is a modified version from https://github.com/Originate/dbg-pds-tensorflow-demo/blob/master/notebooks/02-load-multiple-days-and-prepare-ds.ipynb (MIT License)\n",
    "\n",
    "## Obtaining Data\n",
    "\n",
    "We obtain data for multiple days from the AWS PDS S3 bucket and output a single dataset for intraday price data.\n",
    "\n",
    "https://registry.opendata.aws/deutsche-boerse-pds/\n",
    "\n",
    "### Output dataset \n",
    "\n",
    "- Contains the top 100 stocks by trade volume for the days from 2020-01-01 to 2020-03-31 excluding days with no\n",
    "trades. \n",
    "- We have filled in missing trades to have volume 0 and missing prices by forward filling.\n",
    "- The data is saved into the specified S3 bucket as CSV.\n",
    "\n",
    "```\n",
    "hist_data_intraday/db-pds-2020-q1.csv (columns: sym,open,high,low,close,vol)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get S3 bucket\n",
    "s3bucket=!(aws s3 ls | grep algotrading- | awk  '{print $3}')\n",
    "s3bucket=s3bucket[0]\n",
    "s3bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os.path\n",
    "\n",
    "enable_assert = False \n",
    "\n",
    "# Edit the start/end date and the output folders\n",
    "from_date = '2020-01-01'\n",
    "until_date = '2020-03-31'\n",
    "\n",
    "local_data_folder = 'db-pds/input' # do not end in /\n",
    "output_folder = 'db-pds/output' # do not end in /\n",
    "\n",
    "download_script = 'db-pds/input/download.sh'\n",
    "\n",
    "dates = list(pd.date_range(from_date, until_date, freq='D').strftime('%Y-%m-%d'))\n",
    "\n",
    "! mkdir -p {local_data_folder}\n",
    "\n",
    "# We found it was more reliable to generate a bash script and run it, rather than\n",
    "# run the commands in a python for-loop\n",
    "\n",
    "with open(download_script, 'w') as f:\n",
    "    f.write(\"#!/bin/bash\\n\")\n",
    "    f.write(\"\\nset -euo pipefail\\n\")\n",
    "    f.write(\"\\n# This script was generated to download data for multiple days\\n\")\n",
    "    for date in dates:\n",
    "        success_file =  os.path.join(local_data_folder, date, 'success')\n",
    "\n",
    "        f.write(\"\"\"\n",
    "if [ ! -f {success_file} ]; then\n",
    "\n",
    "    echo \"Getting PDS dataset for date {date}\"        \n",
    "    mkdir -p {local_data_folder}/{date}\n",
    "    aws s3 sync s3://deutsche-boerse-xetra-pds/{date} {local_data_folder}/{date} --no-sign-request\n",
    "    touch {success_file}            \n",
    "else\n",
    "    echo \"PDS dataset for date {date} already exists\"\n",
    "fi\\n\"\"\".format(success_file=success_file, date=date, local_data_folder=local_data_folder))\n",
    "\n",
    "        \n",
    "! chmod +x {download_script}     \n",
    "! head -n 15 {download_script} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!  {download_script}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "\n",
    "We need to ensure we have a data frame of 'common stock' in a suitable form. We take care to filter out any data outside of trading hours also to ensure consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob, os\n",
    "from datetime import datetime\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.figsize'] = (15, 10) # use bigger graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_dirs(data_dirs):\n",
    "    files = []\n",
    "    for data_dir in data_dirs:\n",
    "        files.extend(glob.glob(os.path.join(data_dir, '*.csv')))\n",
    "    return pd.concat(map(pd.read_csv, files))\n",
    "\n",
    "data_dir = local_data_folder + '/'\n",
    "data_subdirs = map(lambda date: data_dir + date, dates)\n",
    "unprocessed_df = load_csv_dirs(data_subdirs)\n",
    "unprocessed_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unprocessed_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want the dates to be comparable to datetime.strptime()\n",
    "unprocessed_df[\"CalcTime\"] = pd.to_datetime(\"1900-01-01 \" + unprocessed_df[\"Time\"])\n",
    "\n",
    "unprocessed_df[\"CalcDateTime\"] = pd.to_datetime(unprocessed_df[\"Date\"] + \" \" + unprocessed_df[\"Time\"])\n",
    "unprocessed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = unprocessed_df[(unprocessed_df.Mnemonic == 'BMW') &\n",
    "                 (unprocessed_df.Date == '2020-01-02') &\n",
    "                 (unprocessed_df.Time == '09:01')]\n",
    "test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = unprocessed_df[(unprocessed_df.Mnemonic == 'BMW') &\n",
    "                 (unprocessed_df.Date == '2020-01-02') &\n",
    "                 (unprocessed_df.Time == '09:02')]\n",
    "test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter common stock\n",
    "# Filter between trading hours 08:00 and 20:00\n",
    "# Exclude auctions (those are with TradeVolume == 0)\n",
    "only_common_stock = unprocessed_df[unprocessed_df.SecurityType == 'Common stock']\n",
    "time_fmt = \"%H:%M\"\n",
    "opening_hours_str = \"08:00\"\n",
    "closing_hours_str = \"20:00\"\n",
    "opening_hours = datetime.strptime(opening_hours_str, time_fmt)\n",
    "closing_hours = datetime.strptime(closing_hours_str, time_fmt)\n",
    "\n",
    "cleaned_common_stock = only_common_stock[(only_common_stock.TradedVolume > 0) & \\\n",
    "                  (only_common_stock.CalcTime >= opening_hours) & \\\n",
    "                  (only_common_stock.CalcTime <= closing_hours)]\n",
    "cleaned_common_stock.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bymnemonic = cleaned_common_stock[['Mnemonic', 'TradedVolume']].groupby(['Mnemonic']).sum()\n",
    "number_of_stocks = 100\n",
    "top = bymnemonic.sort_values(['TradedVolume'], ascending=[0]).head(number_of_stocks)\n",
    "top.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_stocks = list(top.index.values)\n",
    "cleaned_common_stock = cleaned_common_stock[cleaned_common_stock.Mnemonic.isin(top_k_stocks)]\n",
    "cleaned_common_stock.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some notebooks we use a subset of this data, and therefore we export it here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_by_index = cleaned_common_stock.set_index(['Mnemonic', 'CalcDateTime']).sort_index()\n",
    "sorted_by_index.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_empty_days = sorted(list(cleaned_common_stock['Date'].unique()))\n",
    "len(non_empty_days), non_empty_days[0:2], '...', non_empty_days[-3:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "def build_index(non_empty_days, from_time, to_time):\n",
    "    date_ranges = []\n",
    "    for date in non_empty_days:\n",
    "        yyyy, mm, dd = date.split('-')\n",
    "        from_hour, from_min = from_time.split(':')\n",
    "        to_hour, to_min = to_time.split(':')    \n",
    "        t1 = datetime.datetime(int(yyyy), int(mm), int(dd), int(from_hour),int(from_min),0)\n",
    "        t2 = datetime.datetime(int(yyyy), int(mm), int(dd), int(to_hour),int(to_min),0) \n",
    "        date_ranges.append(pd.DataFrame({\"OrganizedDateTime\": pd.date_range(t1, t2, freq='1Min').values}))\n",
    "    agg = pd.concat(date_ranges, axis=0) \n",
    "    agg.index = agg[\"OrganizedDateTime\"]\n",
    "    return agg\n",
    "new_datetime_index = build_index(non_empty_days, opening_hours_str, closing_hours_str)[\"OrganizedDateTime\"].values\n",
    "new_datetime_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_stock_features(input_df, mnemonic, new_time_index):\n",
    "    stock = sorted_by_index.loc[mnemonic].copy()\n",
    "\n",
    "    stock['HasTrade'] = 1.0\n",
    "    \n",
    "    stock = stock.reindex(new_time_index)\n",
    "    \n",
    "    features = ['MinPrice', 'MaxPrice', 'EndPrice', 'StartPrice']\n",
    "    for f in features:\n",
    "        stock[f] = stock[f].fillna(method='ffill')   \n",
    "    \n",
    "    features = ['HasTrade', 'TradedVolume', 'NumberOfTrades']\n",
    "    for f in features:\n",
    "        stock[f] = stock[f].fillna(0.0)\n",
    "    \n",
    "    stock['Mnemonic'] = mnemonic\n",
    "    selected_features = ['Mnemonic', 'MinPrice', 'MaxPrice', 'StartPrice', 'EndPrice', 'HasTrade', 'TradedVolume', 'NumberOfTrades']\n",
    "    return stock[selected_features]\n",
    "\n",
    "\n",
    "bmw = basic_stock_features(sorted_by_index, 'BMW', new_datetime_index)\n",
    "bmw[['TradedVolume']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bmw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = []\n",
    "for stock in top_k_stocks:\n",
    "    stock = basic_stock_features(sorted_by_index, stock, new_datetime_index)\n",
    "    stocks.append(stock)\n",
    "# prepared should contain the numeric features for all top k stocks,\n",
    "# for all days in the interval, for which there were trades (that means excluding weekends and holidays)\n",
    "# for all minutes from 08:00 until 20:00\n",
    "# in minutes without trades the prices from the last available minute are carried forward\n",
    "# trades are filled with zero for such minutes\n",
    "# a new column called HasTrade is introduced to denote the presence of trades\n",
    "prepared = pd.concat(stocks, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared['Mnemonic'].unique(), prepared['Mnemonic'].unique().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del prepared[\"NumberOfTrades\"]\n",
    "del prepared[\"HasTrade\"]\n",
    "prepared.rename(inplace=True,columns={'Mnemonic':'sym', 'StartPrice': 'open', 'EndPrice': 'close', 'TradedVolume': 'vol', 'MinPrice': 'low', 'MaxPrice':'high'})\n",
    "prepared.index=prepared.index.rename('dt')\n",
    "prepared = prepared[['sym', 'open', 'high', 'low', 'close','vol']]\n",
    "prepared.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p {output_folder}\n",
    "prepared.to_csv(output_folder + '/db-pds-2020-q1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws s3 cp {output_folder}/db-pds-2020-q1.csv s3://{s3bucket}/hist_data_intraday/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! echo \"Last run on $(date)\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
